# Multi-Vector Video Search Pipeline

A production-grade video semantic search pipeline implementing the [TwelveLabs Multi-Vector Video Search Guidance](./A%20Guidance%20on%20Multi-Vector%20Video%20Search%20with%20TwelveLabs%20Marengo.pdf). Built with AWS Bedrock Marengo 3.0, featuring dual vector storage backends: **MongoDB Atlas** (single-index) and **Amazon S3 Vectors** (multi-index).

## Live Demo

**Search UI:** https://nyfwaxmgni.us-east-1.awsapprunner.com

---

## üéØ Multi-Vector Search Approaches

### Approach: Multi-Vector Retrieval (Section 3)

**Implementation:** Three separate embedding vectors per video segment, combined at query time.

**Storage Architecture:**
```
Video Segment ‚Üí Three 512d Embeddings:
  ‚îú‚îÄ Visual Embedding      (visual content, scenes, actions)
  ‚îú‚îÄ Audio Embedding       (sounds, music, ambient audio)
  ‚îî‚îÄ Transcription Embedding (spoken words, dialogue)
```

**Advantages:**
- ‚úÖ Preserves modality-specific signal fidelity
- ‚úÖ Transparent, modality-level debuggability
- ‚úÖ Change weights without re-indexing
- ‚úÖ Supports modality-specific optimization
- ‚úÖ Foundation for adaptive architectures

**Drawbacks:**
- ‚ùå 3x storage footprint vs fused embeddings
- ‚ùå 3 vector searches instead of 1
- ‚ùå More complex infrastructure (3 indices)

**When to Use:**
- Production deployments requiring transparency
- Mixed query intent across modalities
- State-of-the-art semantic search
- Modality-specific tuning required

---

## üîÄ Fusion Methods

### 1. Reciprocal Rank Fusion (RRF)

**Formula:**
```
score(d) = Œ£ w_m / (k + rank_m(d))

Where:
  w_m = modality weight
  k = 60 (standard RRF constant)
  rank_m(d) = rank of document d in modality m
```

**Implementation:** `search_client.py:301`

**Characteristics:**
- ‚úÖ **Robust** to score distribution differences
- ‚úÖ **Emphasizes agreement** between modalities
- ‚úÖ **Standard approach** (used by Elasticsearch, etc.)
- ‚úÖ Better for **diverse query distributions**

**Default Weights:**
```python
{
  "visual": 0.8,      # 80% weight on visual ranking
  "audio": 0.1,       # 10% weight on audio ranking
  "transcription": 0.05  # 5% weight on transcription ranking
}
```

**API Usage:**
```python
results = client.search(
    query="person running in park",
    fusion_method="rrf",
    weights={"visual": 0.8, "audio": 0.1, "transcription": 0.05}
)
```

---

### 2. Weighted Score Fusion

**Formula:**
```
score(s) = Œ£ w_m ¬∑ sim(Q_m, E_m(s))

Where:
  w_m = modality weight
  sim() = cosine similarity
  Q_m = query embedding for modality m
  E_m(s) = segment embedding for modality m
```

**Implementation:** `search_client.py:359`

**Characteristics:**
- ‚úÖ **Direct score combination**
- ‚úÖ **Simpler** than RRF
- ‚ö†Ô∏è Sensitive to score distributions
- ‚úÖ Works well with **normalized scores**

**Default Weights:**
```python
{
  "visual": 0.8,
  "audio": 0.1,
  "transcription": 0.1
}
```

**API Usage:**
```python
results = client.search(
    query="person running in park",
    fusion_method="weighted",
    weights={"visual": 0.8, "audio": 0.1, "transcription": 0.1}
)
```

---

### 3. Intent-Based Dynamic Routing (Section 4.3)

**Implementation:** Uses embedding similarity to anchor prompts to automatically compute weights.

**How It Works:**
1. Pre-compute anchor embeddings for each modality (at startup)
2. For each query, compute cosine similarity to each anchor
3. Apply softmax with temperature to get normalized weights

**Formula:**
```
(w_v, w_a, w_t) = softmax(Œ± ¬∑ sim(E_query, [E_AncV, E_AncA, E_AncT]))

Where:
  Œ± = temperature (default: 10.0)
  E_query = query embedding
  E_AncV/A/T = anchor embeddings for visual/audio/transcription
```

**Anchor Prompts:**
```python
VISUAL_ANCHOR = "What appears on screen: people, objects, scenes, actions,
                 clothing, colors, and visual composition of the video."

AUDIO_ANCHOR = "The non-speech audio in the video: music, sound effects,
                ambient sound, and other audio elements."

TRANSCRIPTION_ANCHOR = "The spoken words in the video: dialogue, narration,
                        speech, and what people say."
```

**Implementation:** `search_client.py:136-184`

**Characteristics:**
- ‚úÖ **Query-adaptive** - weights change per query
- ‚úÖ **Deterministic** - same query = same weights
- ‚úÖ **Explainable** - can inspect anchor similarities
- ‚úÖ **No training required** - uses embedding space directly
- ‚úÖ **Fast iteration** - update anchors without retraining

**API Usage:**
```python
response = client.search_dynamic(
    query="explosion with loud bang",
    temperature=10.0  # Higher = more uniform, lower = more decisive
)

print(f"Computed weights: {response['weights']}")
# Output: {"visual": 0.45, "audio": 0.42, "transcription": 0.13}

print(f"Anchor similarities: {response['similarities']}")
# Output: {"visual": 0.78, "audio": 0.75, "transcription": 0.45}
```

**Temperature Effects:**
| Temperature | Behavior | Example Weights (visual, audio, transcription) |
|-------------|----------|-----------------------------------------------|
| `Œ± = 1.0` | Very decisive (sharp distribution) | 0.89, 0.08, 0.03 |
| `Œ± = 10.0` (default) | Balanced adaptation | 0.45, 0.42, 0.13 |
| `Œ± = 50.0` | Uniform (ignores differences) | 0.34, 0.33, 0.33 |

---

## üß† LLM Query Decomposition (Section 3.2.2)

**Purpose:** Decompose complex natural language queries into modality-specific sub-queries for enhanced precision.

**Implementation:** `bedrock_client.py:256-401`

**How It Works:**
1. User provides a natural language query
2. Claude 3 Haiku decomposes it into three distinct queries:
   - **Visual query**: What appears on screen
   - **Audio query**: Non-speech sounds only
   - **Transcription query**: Spoken words and dialogue
3. Each sub-query gets its own embedding
4. Separate vector searches per modality using appropriate embeddings

**Example:**

**Input Query:**
```
"Ross says I take thee Rachel at a wedding"
```

**LLM Decomposition:**
```python
{
  "visual": "Ross at a wedding ceremony, wedding altar, formal attire",
  "audio": "wedding music, ceremony sounds, emotional atmosphere",
  "transcription": "Ross says I take thee Rachel"
}
```

**Model Configuration:**
- **Model:** Claude 3 Haiku (`anthropic.claude-3-haiku-20240307-v1:0`)
- **Temperature:** 0.3 (low for deterministic structured output)
- **Max Tokens:** 500

**API Usage:**
```python
# Enable decomposition with flag
results = client.search(
    query="Ross says I take thee Rachel at a wedding",
    fusion_method="rrf",
    decomposed_queries=client.bedrock.decompose_query(query)
)
```

**Web UI:** Enable "Use LLM Decomposition" toggle

**Characteristics:**
- ‚úÖ **Precision boost** for complex multi-modal queries
- ‚úÖ **Extracts distinct signals** from ambiguous queries
- ‚úÖ **Context-aware expansion** - infers relevant elements
- ‚ö†Ô∏è **Adds latency** (~500ms for LLM call)
- ‚ö†Ô∏è **Requires Bedrock access** to Claude models

**Best For:**
- Complex queries spanning multiple modalities
- Queries where visual/audio/speech elements are intertwined
- When maximum precision is more important than latency

**Not Recommended For:**
- Simple single-modality queries ("red car")
- High-throughput/low-latency requirements
- Cost-sensitive applications (adds LLM inference cost)

---

## ‚öñÔ∏è Modality Weight Configurations

### 1. Fixed Weights (Section 4.1)

**Method:** Manually set or statistically optimized weights applied to all queries.

**Default (Visual-Heavy):**
```python
VISUAL_WEIGHT = 0.8
AUDIO_WEIGHT = 0.1
TRANSCRIPTION_WEIGHT = 0.1
```

**Recommended Configurations by Use Case:**

| Use Case | Visual | Audio | Transcription | Example Query |
|----------|--------|-------|---------------|---------------|
| **Visual-Centric** | 0.80 | 0.10 | 0.10 | "person running", "red car crash" |
| **Dialogue-Focused** | 0.20 | 0.10 | 0.70 | "what did they say about revenue", "find where he mentions the deadline" |
| **Audio Events** | 0.30 | 0.60 | 0.10 | "explosion sound", "alarm ringing", "music playing" |
| **Balanced** | 0.40 | 0.30 | 0.30 | "wedding ceremony", "basketball game" |
| **Speech-Heavy + Visual** | 0.40 | 0.10 | 0.50 | "presenter showing slides", "interview about product" |

**Configuration Methods:**

**1. Environment Variables:**
```bash
export WEIGHT_VISUAL=0.8
export WEIGHT_AUDIO=0.1
export WEIGHT_TRANSCRIPTION=0.1
```

**2. API Parameters:**
```python
results = client.search(
    query="person laughing at joke",
    weights={"visual": 0.4, "audio": 0.3, "transcription": 0.3}
)
```

**3. Web UI Sliders:**
- Adjust visual/audio/transcription sliders in real-time
- Weights automatically normalize to sum to 1.0

**Statistical Optimization (Advanced):**

If you have historical query data with ground truth relevance labels:

```python
from search_optimization import optimize_weights

# Your evaluation dataset
eval_queries = [
    {"query": "person running", "relevant_segments": [...]},
    {"query": "alarm sound", "relevant_segments": [...]},
    # ... more examples
]

# Run grid search or Bayesian optimization
optimal_weights = optimize_weights(
    eval_queries=eval_queries,
    metric="precision@10",  # or "recall@20", "map", etc.
    search_space={
        "visual": (0.1, 0.9),
        "audio": (0.05, 0.5),
        "transcription": (0.05, 0.7)
    }
)

print(optimal_weights)
# Output: {"visual": 0.72, "audio": 0.13, "transcription": 0.15}
```

**Characteristics:**
- ‚úÖ **Simple** - no ML training required
- ‚úÖ **Predictable** - same weights for all queries
- ‚úÖ **Fast** - no per-query computation
- ‚ö†Ô∏è **Not adaptive** - can't adjust to query intent
- ‚ö†Ô∏è **Requires domain knowledge** or labeled data for optimization

---

### 2. Dynamic Routing with Anchors (Section 4.3)

**Method:** Automatically compute weights per query using anchor similarity.

See [Intent-Based Dynamic Routing](#3-intent-based-dynamic-routing-section-43) above for detailed explanation.

**Query-Specific Weight Examples:**

| Query | Visual | Audio | Transcription | Reasoning |
|-------|--------|-------|---------------|-----------|
| "person running in park" | 0.71 | 0.15 | 0.14 | Strong visual signal |
| "explosion with loud bang" | 0.45 | 0.42 | 0.13 | Visual + audio balanced |
| "he says I take thee Rachel" | 0.22 | 0.12 | 0.66 | Heavily speech-focused |
| "wedding ceremony music" | 0.38 | 0.47 | 0.15 | Audio-dominant |
| "red car crash" | 0.68 | 0.18 | 0.14 | Visual with some audio |

**API Usage:**
```python
response = client.search_dynamic(
    query="explosion with loud bang",
    temperature=10.0,
    limit=50
)

# Inspect computed weights
print(f"Query: {query}")
print(f"Visual weight: {response['weights']['visual']:.2f}")
print(f"Audio weight: {response['weights']['audio']:.2f}")
print(f"Transcription weight: {response['weights']['transcription']:.2f}")

# Results
for result in response['results']:
    print(f"Segment {result['segment_id']}: {result['fusion_score']:.3f}")
```

---

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   S3 Bucket     ‚îÇ     ‚îÇ  AWS Lambda      ‚îÇ
‚îÇ   (Videos)      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  (Processing)    ‚îÇ
‚îÇ                 ‚îÇ     ‚îÇ                  ‚îÇ
‚îÇ tl-brice-media/ ‚îÇ     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ WBD_project/    ‚îÇ     ‚îÇ  ‚îÇ  Bedrock   ‚îÇ  ‚îÇ
‚îÇ Videos/Ready/   ‚îÇ     ‚îÇ  ‚îÇ  Marengo   ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ  3.0       ‚îÇ  ‚îÇ
         ‚îÇ              ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
    S3 Trigger          ‚îÇ                  ‚îÇ
    (automatic)         ‚îÇ  Embeddings:     ‚îÇ
                        ‚îÇ  - Visual (512d) ‚îÇ
                        ‚îÇ  - Audio (512d)  ‚îÇ
                        ‚îÇ  - Transcription ‚îÇ
                        ‚îÇ    (512d)        ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                                                 ‚îÇ
         ‚ñº                                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MongoDB Atlas         ‚îÇ               ‚îÇ   Amazon S3 Vectors         ‚îÇ
‚îÇ   (Single Index Mode)   ‚îÇ               ‚îÇ   (Multi-Index Mode)        ‚îÇ
‚îÇ                         ‚îÇ               ‚îÇ                             ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ               ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ  video_embeddings   ‚îÇ ‚îÇ               ‚îÇ ‚îÇ  visual-embeddings      ‚îÇ ‚îÇ
‚îÇ ‚îÇ  (single collection)‚îÇ ‚îÇ               ‚îÇ ‚îÇ  (separate index)       ‚îÇ ‚îÇ
‚îÇ ‚îÇ                     ‚îÇ ‚îÇ               ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ ‚îÇ  modality_type:     ‚îÇ ‚îÇ               ‚îÇ ‚îÇ  audio-embeddings       ‚îÇ ‚îÇ
‚îÇ ‚îÇ  - visual           ‚îÇ ‚îÇ               ‚îÇ ‚îÇ  (separate index)       ‚îÇ ‚îÇ
‚îÇ ‚îÇ  - audio            ‚îÇ ‚îÇ               ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ ‚îÇ  - transcription    ‚îÇ ‚îÇ               ‚îÇ ‚îÇ  transcription-embs     ‚îÇ ‚îÇ
‚îÇ ‚îÇ                     ‚îÇ ‚îÇ               ‚îÇ ‚îÇ  (separate index)       ‚îÇ ‚îÇ
‚îÇ ‚îÇ  HNSW Vector Index  ‚îÇ ‚îÇ               ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ ‚îÇ  + Filter Fields    ‚îÇ ‚îÇ               ‚îÇ  Bucket: brice-video-       ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ               ‚îÇ  search-multimodal          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                                          ‚îÇ
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CloudFront    ‚îÇ     ‚îÇ  AWS App Runner  ‚îÇ
‚îÇ   (CDN)         ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  (Search API)    ‚îÇ
‚îÇ                 ‚îÇ     ‚îÇ                  ‚îÇ
‚îÇ Video streaming ‚îÇ     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ + thumbnails    ‚îÇ     ‚îÇ  ‚îÇ  FastAPI   ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ  + Multi   ‚îÇ  ‚îÇ
                        ‚îÇ  ‚îÇ    Fusion  ‚îÇ  ‚îÇ
                        ‚îÇ  ‚îÇ  + Dynamic ‚îÇ  ‚îÇ
                        ‚îÇ  ‚îÇ    Routing ‚îÇ  ‚îÇ
                        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                        ‚îÇ                  ‚îÇ
                        ‚îÇ  Fusion Methods: ‚îÇ
                        ‚îÇ  - RRF           ‚îÇ
                        ‚îÇ  - Weighted      ‚îÇ
                        ‚îÇ  - Dynamic       ‚îÇ
                        ‚îÇ                  ‚îÇ
                        ‚îÇ  Query Modes:    ‚îÇ
                        ‚îÇ  - LLM Decomp    ‚îÇ
                        ‚îÇ  - Single Query  ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üñ•Ô∏è Search UI Features

The web interface provides comprehensive search capabilities:

### Search Modes

**Multi-Vector Fusion:**
- **RRF** - Reciprocal Rank Fusion (rank-based, most robust)
- **Weighted** - Score-based fusion with adjustable weights
- **Dynamic** - Intent-based routing with automatic weight calculation

**Single Modality:**
- **Visual** - Visual content only (scenes, actions, objects)
- **Audio** - Audio/sound only (music, sound effects, ambient)
- **Speech** - Transcription/dialogue only (spoken words)

### Query Options

- **LLM Decomposition** - Enable/disable query decomposition with Claude
- **Modality Weights** - Real-time sliders for visual/audio/transcription weights
- **Temperature Control** - Adjust softmax temperature for dynamic routing (1-50)

### Backend Toggle

- **MongoDB (Single Index)** - One collection with modality filter (default)
- **S3 Vectors (Multi-Index)** - Separate index per modality

### Result Card Layout

Each search result displays comprehensive match information:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ #1           85%     [VIS]  ‚îÇ  ‚Üê Rank, Confidence %, Dominant Modality
‚îÇ                             ‚îÇ
‚îÇ     [Video Thumbnail]       ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ         0:30 - 1:15         ‚îÇ  ‚Üê Timestamp Range
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  Video Title
  vis: 0.85  aud: 0.12  tra: 0.03  ‚Üê Individual Modality Scores
  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚Üê Visual Score Bars
```

**Key Features:**
- **Ranking Badge** (#1, #2, #3...) - Shows result position
- **Confidence %** - Match confidence (0-100%)
- **Dominant Badge** - Which modality scored highest (VIS/AUD/TRA)
- **Modality Scores** - Detailed breakdown per embedding type
- **Score Visualization** - Visual bars showing relative strengths
- **20 Results per Page** - Focused, high-quality results

---

## üìÅ Project Structure

```
multi-modal-video-search/
‚îú‚îÄ‚îÄ app.py                        # FastAPI web application (search API)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ lambda_function.py        # Lambda handler for video processing
‚îÇ   ‚îú‚îÄ‚îÄ bedrock_client.py         # Bedrock Marengo client + LLM decomposition
‚îÇ   ‚îú‚îÄ‚îÄ mongodb_client.py         # MongoDB embedding storage
‚îÇ   ‚îú‚îÄ‚îÄ s3_vectors_client.py      # S3 Vectors embedding storage & search
‚îÇ   ‚îú‚îÄ‚îÄ search_client.py          # Multi-vector search with all fusion methods
‚îÇ   ‚îî‚îÄ‚îÄ query_fusion.py           # Legacy query fusion script
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îî‚îÄ‚îÄ index.html                # Search UI frontend
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh                 # AWS CLI deployment script
‚îÇ   ‚îú‚îÄ‚îÄ mongodb_setup.md          # MongoDB Atlas setup guide
‚îÇ   ‚îî‚îÄ‚îÄ migrate_to_s3_vectors.py  # Migration script: MongoDB ‚Üí S3 Vectors
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ .env.example                  # Environment variables template
‚îî‚îÄ‚îÄ README.md                     # This file
```

---

## üöÄ Quick Start

### 1. Clone and Setup

```bash
cd multi-modal-video-search

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Copy and configure environment
cp .env.example .env
# Edit .env with your MongoDB URI and other settings
```

### 2. Setup MongoDB Atlas

Follow the detailed guide in [scripts/mongodb_setup.md](scripts/mongodb_setup.md):

1. Create a cluster (free tier M0 works)
2. Create database user and get connection string
3. Create the `video_embeddings` collection with vector index
4. Whitelist IPs (or use 0.0.0.0/0 for testing)
5. Update `MONGODB_URI` in your `.env` file

### 3. Deploy Lambda Function

```bash
# Set MongoDB URI
export MONGODB_URI="your_mongodb_connection_string_here"

# Deploy
./scripts/deploy.sh
```

### 4. Run Search API Locally

```bash
# Start the FastAPI server
python app.py

# Open browser to http://localhost:8000
```

### 5. Process a Video

```bash
# Invoke Lambda
aws lambda invoke \
  --function-name video-embedding-pipeline \
  --region us-east-1 \
  --payload '{"s3_key": "WBD_project/Videos/Ready/sample.mp4", "bucket": "tl-brice-media"}' \
  --cli-binary-format raw-in-base64-out \
  response.json
```

### 6. Search Videos

**Via Web UI:** http://localhost:8000

**Via API:**
```bash
# Simple search with RRF fusion
curl "http://localhost:8000/api/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "person running in park",
    "fusion_method": "rrf",
    "limit": 10
  }'

# Dynamic routing search
curl "http://localhost:8000/api/search/dynamic" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "explosion with loud bang",
    "temperature": 10.0,
    "limit": 10
  }'

# With LLM decomposition
curl "http://localhost:8000/api/search" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Ross says I take thee Rachel at a wedding",
    "use_decomposition": true,
    "fusion_method": "rrf",
    "limit": 10
  }'
```

---

## üìä MongoDB Schema

### Single Collection: `video_embeddings`

All embeddings stored in one collection with `modality_type` field for filtering.

### Document Schema

```json
{
  "_id": "ObjectId",
  "video_id": "string - unique video identifier",
  "segment_id": "int - segment index within video",
  "modality_type": "string - 'visual' | 'audio' | 'transcription'",
  "s3_uri": "string - s3://bucket/key",
  "embedding": "[float] - 512-dimensional vector",
  "start_time": "float - segment start (seconds)",
  "end_time": "float - segment end (seconds)",
  "created_at": "datetime - document creation time"
}
```

### Vector Index Definition

```json
{
  "fields": [
    { "type": "vector", "path": "embedding", "numDimensions": 512, "similarity": "cosine" },
    { "type": "filter", "path": "modality_type" },
    { "type": "filter", "path": "video_id" }
  ]
}
```

---

## üß™ API Reference

### VideoSearchClient (search_client.py)

```python
from src.search_client import VideoSearchClient

client = VideoSearchClient(
    mongodb_uri="mongodb+srv://...",
    database_name="video_search"
)

# ============ RRF Fusion Search ============
results = client.search(
    query="person running",
    fusion_method="rrf",
    weights={"visual": 0.8, "audio": 0.1, "transcription": 0.1},
    limit=10
)

# ============ Weighted Fusion Search ============
results = client.search(
    query="person running",
    fusion_method="weighted",
    weights={"visual": 0.8, "audio": 0.1, "transcription": 0.1},
    limit=10
)

# ============ Dynamic Intent Routing ============
response = client.search_dynamic(
    query="explosion with loud bang",
    temperature=10.0,
    limit=10
)
print(f"Computed weights: {response['weights']}")
print(f"Anchor similarities: {response['similarities']}")

# ============ With LLM Query Decomposition ============
decomposed = client.bedrock.decompose_query("Ross says I take thee Rachel at a wedding")
print(f"Visual: {decomposed['visual']}")
print(f"Audio: {decomposed['audio']}")
print(f"Transcription: {decomposed['transcription']}")

results = client.search(
    query="Ross says I take thee Rachel at a wedding",
    decomposed_queries=decomposed,
    fusion_method="rrf",
    limit=10
)

# ============ Single Modality Search ============
results = client.search(
    query="person running",
    modalities=["visual"],  # Only search visual modality
    limit=10
)
```

### BedrockMarengoClient (bedrock_client.py)

```python
from src.bedrock_client import BedrockMarengoClient

client = BedrockMarengoClient(region="us-east-1")

# ============ Generate Video Embeddings ============
result = client.get_video_embeddings(
    bucket="tl-brice-media",
    s3_key="WBD_project/Videos/file.mp4",
    embedding_types=["visual", "audio", "transcription"]
)

# ============ Generate Query Embedding ============
query_result = client.get_text_query_embedding("a car driving fast")

# ============ LLM Query Decomposition ============
decomposed = client.decompose_query("Ross says I take thee Rachel at a wedding")
print(decomposed)
# {
#   "original_query": "Ross says I take thee Rachel at a wedding",
#   "visual": "Ross at a wedding ceremony, wedding altar, formal attire",
#   "audio": "wedding music, ceremony sounds, emotional atmosphere",
#   "transcription": "Ross says I take thee Rachel"
# }
```

---

## üí∞ Cost Estimation

Based on Marengo 3.0 pricing:

| Component | Price | Notes |
|-----------|-------|-------|
| Video embedding | $0.0007/second | For video processing |
| Text query embedding | Included | No additional cost |
| LLM decomposition (optional) | ~$0.0001/query | Claude 3 Haiku (500 tokens) |

**Example Costs:**
- **1 hour video processing:** 3,600 sec √ó $0.0007 = **$2.52**
- **1,000 searches (no decomposition):** **$0** (text embeddings included)
- **1,000 searches (with decomposition):** ~**$0.10** (LLM calls)

---

## üîß Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MONGODB_URI` | Required | MongoDB connection string |
| `MONGODB_DATABASE` | `video_search` | Database name |
| `AWS_REGION` | `us-east-1` | AWS region for Bedrock |
| `S3_BUCKET` | `tl-brice-media` | S3 bucket for videos |
| `CLOUDFRONT_DOMAIN` | `d2h48upmn4e6uy.cloudfront.net` | CloudFront domain |
| `WEIGHT_VISUAL` | `0.8` | Default visual weight (fixed mode) |
| `WEIGHT_AUDIO` | `0.1` | Default audio weight (fixed mode) |
| `WEIGHT_TRANSCRIPTION` | `0.1` | Default transcription weight (fixed mode) |

---

## üêõ Troubleshooting

### Lambda Timeout

- Default timeout is 15 minutes (900 seconds)
- For very long videos (>2 hours), consider splitting into segments
- Increase memory to 2048MB or higher for faster processing

### Vector Search Returns No Results

1. Verify index is in **Active** state in Atlas UI
2. Check embedding dimensions match (512)
3. Ensure collection has documents
4. Verify filter field values match exactly

### LLM Decomposition Fails

1. Verify Bedrock access to Claude 3 Haiku model
2. Check model ID is correct: `anthropic.claude-3-haiku-20240307-v1:0`
3. Ensure AWS credentials have `bedrock:InvokeModel` permission
4. Check CloudWatch logs for detailed error messages

### Connection Errors

1. Verify MongoDB Atlas IP whitelist includes Lambda/App Runner IPs
2. Check connection string format
3. For testing, use 0.0.0.0/0 in Atlas Network Access

---

## üìö References

- [TwelveLabs Multi-Vector Guidance](./A%20Guidance%20on%20Multi-Vector%20Video%20Search%20with%20TwelveLabs%20Marengo.pdf) - Complete whitepaper
- [MongoDB Atlas Vector Search](https://www.mongodb.com/docs/atlas/atlas-vector-search/)
- [Amazon S3 Vectors Documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-vectors.html)
- [AWS Bedrock Marengo](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-marengo.html)
- [Claude 3 Models](https://docs.anthropic.com/claude/docs/models-overview)

---

## üìù License

Internal use only. All rights reserved.
